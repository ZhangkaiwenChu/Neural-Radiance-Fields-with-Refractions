{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8096bd69-06cb-45ba-947b-9256fc000e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import imageio\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "997f21e3-98fa-4606-99f9-3c6cba80f83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_llff import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7b186f6-12f6-4b15-ae60-61a66d731af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "np.random.seed(0)\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c1e80f-d942-48c1-9df5-3bc1b1d9d0f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0efb0e76-9a75-438b-bc6e-104cacef9cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = config_parser()\n",
    "args = parser.parse_args('--config C:/Users/chuzh/Study/CIS565/final/Neural-Radiance-Fields-with-Refractions/code/configs/fern.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb528a90-b17b-49a8-a355-dded4ed64c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded image data (378, 504, 3, 20) [378.         504.         407.56579161]\n",
      "Loaded ./data/nerf_llff_data/fern 16.985296178676084 80.00209740336334\n",
      "recentered (3, 5)\n",
      "[[ 1.0000000e+00  0.0000000e+00  0.0000000e+00  1.4901161e-09]\n",
      " [ 0.0000000e+00  1.0000000e+00 -1.8730975e-09 -9.6857544e-09]\n",
      " [-0.0000000e+00  1.8730975e-09  1.0000000e+00  0.0000000e+00]]\n",
      "Data:\n",
      "(20, 3, 5) (20, 378, 504, 3) (20, 2)\n",
      "HOLDOUT view is 12\n",
      "Loaded llff (20, 378, 504, 3) (120, 3, 5) [378.     504.     407.5658] ./data/nerf_llff_data/fern\n",
      "Auto LLFF holdout, 8\n",
      "DEFINING BOUNDS\n",
      "NEAR FAR 0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "K = None\n",
    "if args.dataset_type == 'llff':\n",
    "    images, poses, bds, render_poses, i_test = load_llff_data(args.datadir, args.factor,\n",
    "                                                                  recenter=True, bd_factor=.75,\n",
    "                                                                  spherify=args.spherify)\n",
    "    hwf = poses[0,:3,-1]\n",
    "    poses = poses[:,:3,:4]\n",
    "    print('Loaded llff', images.shape, render_poses.shape, hwf, args.datadir)\n",
    "    if not isinstance(i_test, list):\n",
    "        i_test = [i_test]\n",
    "\n",
    "    if args.llffhold > 0:\n",
    "        print('Auto LLFF holdout,', args.llffhold)\n",
    "        i_test = np.arange(images.shape[0])[::args.llffhold]\n",
    "\n",
    "    i_val = i_test\n",
    "    i_train = np.array([i for i in np.arange(int(images.shape[0])) if\n",
    "                        (i not in i_test and i not in i_val)])\n",
    "\n",
    "    print('DEFINING BOUNDS')\n",
    "    if args.no_ndc:\n",
    "        near = np.ndarray.min(bds) * .9\n",
    "        far = np.ndarray.max(bds) * 1.\n",
    "            \n",
    "    else:\n",
    "        near = 0.\n",
    "        far = 1.\n",
    "    print('NEAR FAR', near, far)\n",
    "\n",
    "elif args.dataset_type == 'blender':\n",
    "    images, poses, render_poses, hwf, i_split = load_blender_data(args.datadir, args.half_res, args.testskip)\n",
    "    print('Loaded blender', images.shape, render_poses.shape, hwf, args.datadir)\n",
    "    i_train, i_val, i_test = i_split\n",
    "\n",
    "    near = 2.\n",
    "    far = 6.\n",
    "\n",
    "    if args.white_bkgd:\n",
    "        images = images[...,:3]*images[...,-1:] + (1.-images[...,-1:])\n",
    "    else:\n",
    "        images = images[...,:3]\n",
    "\n",
    "elif args.dataset_type == 'LINEMOD':\n",
    "    images, poses, render_poses, hwf, K, i_split, near, far = load_LINEMOD_data(args.datadir, args.half_res, args.testskip)\n",
    "    print(f'Loaded LINEMOD, images shape: {images.shape}, hwf: {hwf}, K: {K}')\n",
    "    print(f'[CHECK HERE] near: {near}, far: {far}.')\n",
    "    i_train, i_val, i_test = i_split\n",
    "\n",
    "    if args.white_bkgd:\n",
    "        images = images[...,:3]*images[...,-1:] + (1.-images[...,-1:])\n",
    "    else:\n",
    "        images = images[...,:3]\n",
    "\n",
    "elif args.dataset_type == 'deepvoxels':\n",
    "\n",
    "    images, poses, render_poses, hwf, i_split = load_dv_data(scene=args.shape,\n",
    "                                                                 basedir=args.datadir,\n",
    "                                                                 testskip=args.testskip)\n",
    "\n",
    "    print('Loaded deepvoxels', images.shape, render_poses.shape, hwf, args.datadir)\n",
    "    i_train, i_val, i_test = i_split\n",
    "\n",
    "    hemi_R = np.mean(np.linalg.norm(poses[:,:3,-1], axis=-1))\n",
    "    near = hemi_R-1.\n",
    "    far = hemi_R+1.\n",
    "\n",
    "else:\n",
    "    print('Unknown dataset type', args.dataset_type, 'exiting')\n",
    "\n",
    "# Cast intrinsics to right types\n",
    "H, W, focal = hwf\n",
    "H, W = int(H), int(W)\n",
    "hwf = [H, W, focal]\n",
    "\n",
    "if K is None:\n",
    "    K = np.array([\n",
    "        [focal, 0, 0.5*W],\n",
    "        [0, focal, 0.5*H],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "\n",
    "if args.render_test:\n",
    "    render_poses = np.array(poses[i_test])\n",
    "\n",
    "# Create log dir and copy the config file\n",
    "basedir = args.basedir\n",
    "expname = args.expname\n",
    "os.makedirs(os.path.join(basedir, expname), exist_ok=True)\n",
    "f = os.path.join(basedir, expname, 'args.txt')\n",
    "with open(f, 'w') as file:\n",
    "    for arg in sorted(vars(args)):\n",
    "        attr = getattr(args, arg)\n",
    "        file.write('{} = {}\\n'.format(arg, attr))\n",
    "if args.config is not None:\n",
    "    f = os.path.join(basedir, expname, 'config.txt')\n",
    "    with open(f, 'w') as file:\n",
    "        file.write(open(args.config, 'r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2af6309-5650-43be-8acd-ae85715e0882",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40a996f3-9fbf-4fa6-b04f-07483c338697",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_rand = args.N_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b5f3d27-f322-45fe-9330-107897cf0953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get rays\n",
      "done, concats\n",
      "shuffle rays\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print('get rays')\n",
    "rays = np.stack([get_rays_np(H, W, K, p) for p in poses[:,:3,:4]], 0) # [N, ro+rd, H, W, 3]\n",
    "print('done, concats')\n",
    "rays_rgb = np.concatenate([rays, images[:,None]], 1) # [N, ro+rd+rgb, H, W, 3]\n",
    "rays_rgb = np.transpose(rays_rgb, [0,2,3,1,4]) # [N, H, W, ro+rd+rgb, 3]\n",
    "rays_rgb = np.stack([rays_rgb[i] for i in i_train], 0) # train images only\n",
    "rays_rgb = np.reshape(rays_rgb, [-1,3,3]) # [(N-1)*H*W, ro+rd+rgb, 3]\n",
    "rays_rgb = rays_rgb.astype(np.float32)\n",
    "print('shuffle rays')\n",
    "np.random.shuffle(rays_rgb)\n",
    "\n",
    "print('done')\n",
    "i_batch = 0\n",
    "\n",
    "images = torch.Tensor(images).to(device)\n",
    "poses = torch.Tensor(poses).to(device)\n",
    "rays_rgb = torch.Tensor(rays_rgb).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4579e44d-b387-403d-941b-4cb800c5e66c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a117b51-1c75-4c58-8cb2-fddfa4e36961",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeRF(nn.Module):\n",
    "    def __init__(self, in_pos = 3, in_view = 3, hidden = 256):\n",
    "        super(NeRF, self).__init__()\n",
    "        self.in_pos = in_pos\n",
    "        self.in_view = in_view\n",
    "        self.l1 = nn.Linear(in_pos, hidden)\n",
    "        self.l2 = nn.Linear(hidden, hidden)\n",
    "        self.l3 = nn.Linear(hidden, hidden)\n",
    "        self.l4 = nn.Linear(hidden, hidden)        \n",
    "        self.l5 = nn.Linear(in_pos + hidden, hidden)\n",
    "        self.l6 = nn.Linear(hidden, hidden)    \n",
    "        self.l7 = nn.Linear(hidden, hidden)   \n",
    "        self.l8 = nn.Linear(hidden, hidden)   \n",
    "        self.l9 = nn.Linear(hidden, hidden)   \n",
    "        self.l0 = nn.Linear(hidden + in_view, hidden//2)\n",
    "        self.rgb = nn.Linear(hidden//2, 3)\n",
    "        self.sigma = nn.Linear(hidden, 1)\n",
    "        self.r = nn.ReLU()\n",
    "        self.s = nn.Sigmoid()\n",
    "        \n",
    "        self.o1 = nn.Linear(in_pos + in_view, hidden)\n",
    "        self.o2 = nn.Linear(hidden, hidden)\n",
    "        self.o3 = nn.Linear(hidden, hidden)\n",
    "        self.o4 = nn.Linear(in_pos + in_view + hidden, hidden)\n",
    "        self.o5 = nn.Linear(hidden, hidden)\n",
    "        self.o6 = nn.Linear(hidden, hidden//2)\n",
    "        self.out = nn.Linear(hidden//2, 3)\n",
    "    \n",
    "    def nerf(self, pos,view):\n",
    "        out = self.r(self.l1(pos))\n",
    "        out = self.r(self.l2(out))\n",
    "        out = self.r(self.l3(out))\n",
    "        out = self.r(self.l4(out))\n",
    "        out = self.r(self.l5(torch.cat([pos, out], -1)))\n",
    "        out = self.r(self.l6(out))\n",
    "        out = self.r(self.l7(out))\n",
    "        out = self.r(self.l8(out))\n",
    "        sigma = self.sigma(out)\n",
    "        out = self.l9(out)\n",
    "        out = self.r(self.l0(torch.cat([view, out], -1)))\n",
    "        rgb = self.s(self.rgb(out))\n",
    "        return torch.cat([rgb, sigma], -1)\n",
    "    \n",
    "    def offset(self, x):\n",
    "        pos, view = torch.split(x, [self.in_pos, self.in_view], dim=-1)\n",
    "        out = self.r(self.o1(x))\n",
    "        out = self.r(self.o2(out))\n",
    "        out = self.r(self.o3(out))\n",
    "        out = self.r(self.o4(torch.cat([x, out], -1)))\n",
    "        out = self.r(self.o5(out))\n",
    "        out = self.r(self.o6(out))\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pos, view = torch.split(x, [self.in_pos, self.in_view], dim=-1)\n",
    "        offset = self.offset(x)\n",
    "        return self.nerf(pos + offset, view)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef9a903-39a1-4b58-a510-72d5c45cc9f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Render Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ca59834-0afd-45a1-9de7-d41fa13bb32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(H, W, K, chunk = 1024*32, rays = None, c2w = None, ndc = True, near = 0., far = 1., **kwargs):\n",
    "    if c2w is not None:\n",
    "        rays_o, rays_d = get_rays(H, W, K, c2w)\n",
    "    else:\n",
    "        rays_o, rays_d = rays\n",
    "        \n",
    "    viewdirs = rays_d\n",
    "    viewdirs = viewdirs / torch.norm(viewdirs, dim=-1, keepdim=True)\n",
    "    viewdirs = torch.reshape(viewdirs, [-1,3]).float()\n",
    "    \n",
    "    shape = rays_d.shape\n",
    "    if ndc:\n",
    "        rays_o, rays_d = ndc_rays(H, W, K[0][0], 1., rays_o, rays_d)\n",
    "    \n",
    "    rays_o = torch.reshape(rays_o, [-1,3]).float()\n",
    "    rays_d = torch.reshape(rays_d, [-1,3]).float()\n",
    "    \n",
    "    near, far = near * torch.ones_like(rays_d[...,:1]), far * torch.ones_like(rays_d[...,:1])\n",
    "    rays = torch.cat([rays_o, rays_d, near, far], -1)\n",
    "    rays = torch.cat([rays, viewdirs], -1)\n",
    "    \n",
    "    all_ret = batchify_rays(rays, chunk, **kwargs)\n",
    "    for k in all_ret:\n",
    "        k_sh = list(shape[:-1]) + list(all_ret[k].shape[1:])\n",
    "        all_ret[k] = torch.reshape(all_ret[k], k_sh)\n",
    "\n",
    "    k_extract = ['rgb_map', 'disp_map', 'acc_map', 'dx']\n",
    "    ret_list = [all_ret[k] for k in k_extract]\n",
    "    ret_dict = {k : all_ret[k] for k in all_ret if k not in k_extract}\n",
    "    return ret_list + [ret_dict]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba9f2780-00db-4f16-84fe-b49ecdc8e8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify_rays(rays_flat, chunk=1024*32, **kwargs):\n",
    "    \"\"\"Render rays in smaller minibatches to avoid OOM.\n",
    "    \"\"\"\n",
    "    all_ret = {}\n",
    "    for i in range(0, rays_flat.shape[0], chunk):\n",
    "        ret = render_rays(rays_flat[i:i+chunk], **kwargs)\n",
    "        for k in ret:\n",
    "            if k not in all_ret:\n",
    "                all_ret[k] = []\n",
    "            all_ret[k].append(ret[k])\n",
    "\n",
    "    all_ret = {k : torch.cat(all_ret[k], 0) for k in all_ret}\n",
    "    return all_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "735de496-a4ed-4aa8-bbd9-219a3efa648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_rays(ray_batch,\n",
    "                network_fn,\n",
    "                network_query_fn,\n",
    "                N_samples,\n",
    "                retraw=False,\n",
    "                lindisp=False,\n",
    "                perturb=0.,\n",
    "                white_bkgd=False,\n",
    "                raw_noise_std=0.,\n",
    "                verbose=False,\n",
    "                offset=None):\n",
    "    N_rays = ray_batch.shape[0]\n",
    "    rays_o, rays_d = ray_batch[:,0:3], ray_batch[:,3:6]\n",
    "    bounds = torch.reshape(ray_batch[...,6:8], [-1,1,2])\n",
    "    near, far = bounds[...,0], bounds[...,1] # [-1,1]\n",
    "    viewdirs = ray_batch[:,8:11]\n",
    "    \n",
    "    t_vals = torch.linspace(0., 1., steps=N_samples)\n",
    "    if not lindisp:\n",
    "        z_vals = near * (1.-t_vals) + far * (t_vals)\n",
    "    else:\n",
    "        z_vals = 1./(1./near * (1.-t_vals) + 1./far * (t_vals))\n",
    "    z_vals = z_vals.expand([N_rays, N_samples])\n",
    "    \n",
    "    if perturb > 0.:\n",
    "        # get intervals between samples\n",
    "        mids = .5 * (z_vals[...,1:] + z_vals[...,:-1])\n",
    "        upper = torch.cat([mids, z_vals[...,-1:]], -1)\n",
    "        lower = torch.cat([z_vals[...,:1], mids], -1)\n",
    "        # stratified samples in those intervals\n",
    "        t_rand = torch.rand(z_vals.shape)\n",
    "        \n",
    "        z_vals = lower + (upper - lower) * t_rand\n",
    "    \n",
    "    pos = rays_o[...,None,:] + rays_d[...,None,:] * z_vals[...,:,None]\n",
    "    \n",
    "    raw = network_query_fn(pos, viewdirs, network_fn)\n",
    "    dx = network_query_fn(pos, viewdirs, offset)\n",
    "    rgb_map, disp_map, acc_map, weights, depth_map = raw2outputs(raw, z_vals, rays_d, raw_noise_std, white_bkgd)\n",
    "    \n",
    "    ret = {'rgb_map' : rgb_map, 'disp_map' : disp_map, 'acc_map' : acc_map, 'dx':dx}\n",
    "    if retraw:\n",
    "        ret['raw'] = raw\n",
    "    \n",
    "    for k in ret:\n",
    "        if (torch.isnan(ret[k]).any() or torch.isinf(ret[k]).any()):\n",
    "            print(f\"! [Numerical Error] {k} contains nan or inf.\")\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdf61b00-1fa8-4360-ab2b-9a73c4954d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw2outputs(raw, z_vals, rays_d, raw_noise_std=0, white_bkgd=False):\n",
    "    \n",
    "    raw2alpha = lambda raw, dists, act_fn=F.relu: 1.-torch.exp(-act_fn(raw)*dists)\n",
    "    \n",
    "    dists = z_vals[...,1:] - z_vals[...,:-1]\n",
    "    dists = torch.cat([dists, torch.Tensor([1e10]).expand(dists[...,:1].shape)], -1)\n",
    "    \n",
    "    dists = dists * torch.norm(rays_d[...,None,:], dim=-1)\n",
    "    \n",
    "    rgb = raw[...,:3]\n",
    "    noise = 0.\n",
    "    if raw_noise_std > 0.:\n",
    "        noise = torch.randn(raw[...,3].shape) * raw_noise_std\n",
    "    \n",
    "    alpha = raw2alpha(raw[...,3] + noise, dists)\n",
    "    weights = alpha * torch.cumprod(torch.cat([torch.ones((alpha.shape[0], 1)), 1.-alpha + 1e-10], -1), -1)[:, :-1]\n",
    "    rgb_map = torch.sum(weights[...,None] * rgb, -2)\n",
    "    \n",
    "    depth_map = torch.sum(weights * z_vals, -1)\n",
    "    disp_map = 1./torch.max(1e-10 * torch.ones_like(depth_map), depth_map / torch.sum(weights, -1))\n",
    "    acc_map = torch.sum(weights, -1)\n",
    "\n",
    "    if white_bkgd:\n",
    "        rgb_map = rgb_map + (1.-acc_map[...,None])\n",
    "\n",
    "    return rgb_map, disp_map, acc_map, weights, depth_map    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2c7d544-b43f-4849-adcd-7c6aa9d5efba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_path(render_poses, hwf, K, chunk, render_kwargs, gt_imgs=None, savedir=None, render_factor=0):\n",
    "\n",
    "    H, W, focal = hwf\n",
    "\n",
    "    if render_factor!=0:\n",
    "        # Render downsampled for speed\n",
    "        H = H//render_factor\n",
    "        W = W//render_factor\n",
    "        focal = focal/render_factor\n",
    "\n",
    "    rgbs = []\n",
    "    disps = []\n",
    "\n",
    "    t = time.time()\n",
    "    for i, c2w in enumerate(tqdm(render_poses)):\n",
    "        print(i, time.time() - t)\n",
    "        t = time.time()\n",
    "        rgb, disp, acc,dx, _ = render(H, W, K, chunk=chunk, c2w=c2w[:3,:4], **render_kwargs)\n",
    "        rgbs.append(rgb.cpu().numpy())\n",
    "        disps.append(disp.cpu().numpy())\n",
    "        if i==0:\n",
    "            print(rgb.shape, disp.shape)\n",
    "\n",
    "        \"\"\"\n",
    "        if gt_imgs is not None and render_factor==0:\n",
    "            p = -10. * np.log10(np.mean(np.square(rgb.cpu().numpy() - gt_imgs[i])))\n",
    "            print(p)\n",
    "        \"\"\"\n",
    "\n",
    "        if savedir is not None:\n",
    "            rgb8 = to8b(rgbs[-1])\n",
    "            filename = os.path.join(savedir, '{:03d}.png'.format(i))\n",
    "            imageio.imwrite(filename, rgb8)\n",
    "\n",
    "\n",
    "    rgbs = np.stack(rgbs, 0)\n",
    "    disps = np.stack(disps, 0)\n",
    "\n",
    "    return rgbs, disps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e40128-2e58-44d8-bded-1fc5afec05f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Batchify Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c570d61-cd30-4441-ba9c-84a3583bc49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(fn, chunk):\n",
    "    \"\"\"Constructs a version of 'fn' that applies to smaller batches.\n",
    "    \"\"\"\n",
    "    if chunk is None:\n",
    "        return fn\n",
    "    def ret(inputs):\n",
    "        return torch.cat([fn(inputs[i:i+chunk]) for i in range(0, inputs.shape[0], chunk)], 0)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def run_network(inputs, viewdirs, fn, netchunk=1024*64):\n",
    "    \"\"\"Prepares inputs and applies network 'fn'.\n",
    "    \"\"\"\n",
    "    inputs_flat = torch.reshape(inputs, [-1, inputs.shape[-1]])\n",
    "    input_dirs = viewdirs[:,None].expand(inputs.shape)\n",
    "    input_dirs_flat = torch.reshape(input_dirs, [-1, input_dirs.shape[-1]])\n",
    "    \n",
    "    out = torch.cat([inputs_flat, input_dirs_flat], -1)\n",
    "     \n",
    "    outputs_flat = batchify(fn, netchunk)(out)\n",
    "\n",
    "    outputs = torch.reshape(outputs_flat, list(inputs.shape[:-1]) + [outputs_flat.shape[-1]])\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6deb7b-c7c9-4639-a2fe-479b47856730",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bb340bb-ee00-4b8c-928a-d7f2d25d6f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.chunk = int(1e+20)\n",
    "in_pos = 3\n",
    "in_view = 3\n",
    "hidden = args.netwidth\n",
    "model = NeRF(in_pos, in_view, hidden)\n",
    "grad_vars = list(model.parameters())\n",
    "\n",
    "network_query_fn = lambda inputs, viewdirs, network_fn : run_network(inputs, viewdirs, network_fn,\n",
    "                                                                     netchunk=args.netchunk)\n",
    "optimizer = torch.optim.Adam(params=grad_vars, lr=args.lrate, betas=(0.9, 0.999))\n",
    "\n",
    "start = 0\n",
    "basedir = args.basedir\n",
    "expname = args.expname\n",
    "\n",
    "render_kwargs_train = {\n",
    "        'network_query_fn' : network_query_fn,\n",
    "        'perturb' : args.perturb,\n",
    "        'N_samples' : args.N_samples,\n",
    "        'network_fn' : model,\n",
    "        'white_bkgd' : args.white_bkgd,\n",
    "        'raw_noise_std' : args.raw_noise_std,\n",
    "        'offset' : model.offset,\n",
    "    }\n",
    "\n",
    "if args.dataset_type != 'llff' or args.no_ndc:\n",
    "    print('Not ndc!')\n",
    "    render_kwargs_train['ndc'] = False\n",
    "    render_kwargs_train['lindisp'] = args.lindisp\n",
    "    \n",
    "render_kwargs_test = {k : render_kwargs_train[k] for k in render_kwargs_train}\n",
    "render_kwargs_test['perturb'] = False\n",
    "render_kwargs_test['raw_noise_std'] = 0.\n",
    "\n",
    "global_step = start\n",
    "bds_dict = {\n",
    "    'near' : near,\n",
    "    'far' : far,\n",
    "}\n",
    "render_kwargs_train.update(bds_dict)\n",
    "render_kwargs_test.update(bds_dict)\n",
    "\n",
    "render_poses = torch.Tensor(render_poses).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711cae83-cffe-4a5d-9d8f-e5592baf695f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0988fe67-89d5-4f29-9264-6fadea5865e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin\n",
      "TRAIN views are [ 1  2  3  4  5  6  7  9 10 11 12 13 14 15 17 18 19]\n",
      "TEST views are [ 0  8 16]\n",
      "VAL views are [ 0  8 16]\n"
     ]
    }
   ],
   "source": [
    "N_iters = 200000 + 1\n",
    "print('Begin')\n",
    "print('TRAIN views are', i_train)\n",
    "print('TEST views are', i_test)\n",
    "print('VAL views are', i_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7d3371d-3382-400b-a9c0-98d1f636b8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                                                                                                                                     | 1001/200000 [02:55<9:48:02,  5.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 1000 Loss: 0.02054758556187153  PSNR: 16.964746475219727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▋                                                                                                                                                                    | 2001/200000 [05:51<9:42:56,  5.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 2000 Loss: 0.016658365726470947  PSNR: 17.884098052978516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██▍                                                                                                                                                                   | 3001/200000 [08:48<9:45:00,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 3000 Loss: 0.013820972293615341  PSNR: 18.70074462890625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██▋                                                                                                                                                                   | 3163/200000 [09:16<9:13:24,  5.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle data after an epoch!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|███▎                                                                                                                                                                  | 4001/200000 [11:44<9:37:45,  5.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 4000 Loss: 0.0132161108776927  PSNR: 18.88377571105957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|████▏                                                                                                                                                                 | 5001/200000 [14:41<9:36:32,  5.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 5000 Loss: 0.012807578779757023  PSNR: 19.009363174438477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|████▉                                                                                                                                                                 | 6001/200000 [17:37<9:25:19,  5.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 6000 Loss: 0.013650982640683651  PSNR: 18.72576141357422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|█████▎                                                                                                                                                                | 6326/200000 [18:34<8:53:16,  6.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle data after an epoch!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█████▊                                                                                                                                                                | 7001/200000 [20:32<9:22:25,  5.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 7000 Loss: 0.011868666857481003  PSNR: 19.339645385742188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|██████▋                                                                                                                                                               | 8001/200000 [23:26<9:18:00,  5.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 8000 Loss: 0.010971888899803162  PSNR: 19.681406021118164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███████▍                                                                                                                                                              | 9001/200000 [26:23<9:24:25,  5.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 9000 Loss: 0.011066818609833717  PSNR: 19.63853645324707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███████▉                                                                                                                                                              | 9489/200000 [27:49<8:51:32,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle data after an epoch!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████████▎                                                                                                                                                             | 9999/200000 [29:19<9:18:05,  5.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Iter: 10000 Loss: 0.008641821332275867  PSNR: 20.732513427734375\n",
      "test poses shape torch.Size([3, 3, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                                                                                        | 0/3 [00:00<?, ?it/s]\u001b[AC:\\Users\\chuzh\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3191.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0009968280792236328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                        | 0/3 [00:11<?, ?it/s]\n",
      "  5%|████████▎                                                                                                                                                             | 9999/200000 [29:31<9:20:56,  5.65it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_45316\\1212880991.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test poses shape'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m             \u001b[0mrender_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mposes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhwf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrender_kwargs_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgt_imgs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msavedir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtestsavedir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Saved test set'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_45316\\3084206022.py\u001b[0m in \u001b[0;36mrender_path\u001b[1;34m(render_poses, hwf, K, chunk, render_kwargs, gt_imgs, savedir, render_factor)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mrgb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc2w\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mc2w\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mrender_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mrgbs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mdisps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdisp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "start = start + 1\n",
    "for i in trange(start, N_iters):\n",
    "    time0 = time.time()\n",
    "    batch = rays_rgb[i_batch:i_batch+N_rand]\n",
    "    batch = torch.transpose(batch, 0, 1)\n",
    "    batch_rays, target_s = batch[:2], batch[2]\n",
    "    \n",
    "    i_batch += N_rand\n",
    "    if i_batch >= rays_rgb.shape[0]:\n",
    "        print(\"Shuffle data after an epoch!\")\n",
    "        rand_idx = torch.randperm(rays_rgb.shape[0])\n",
    "        rays_rgb = rays_rgb[rand_idx]\n",
    "        i_batch = 0\n",
    "    rgb, disp, acc, dx, extras = render(H, W, K, chunk=args.chunk, rays=batch_rays,\n",
    "                                                verbose=i < 10, retraw=True,\n",
    "                                                **render_kwargs_train)\n",
    "    optimizer.zero_grad()\n",
    "    img_loss = img2mse(rgb, target_s)\n",
    "    loss = img_loss + 1e-5 * torch.linalg.vector_norm(dx)\n",
    "    psnr = mse2psnr(img_loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "    decay_rate = 0.1\n",
    "    decay_steps = args.lrate_decay * 1000\n",
    "    new_lrate = args.lrate * (decay_rate ** (global_step / decay_steps))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lrate\n",
    "        \n",
    "    if i%args.i_print==0:\n",
    "        tqdm.write(f\"[TRAIN] Iter: {i} Loss: {loss.item()}  PSNR: {psnr.item()}\")\n",
    "            \n",
    "    if i%args.i_video==0 and i > 0:\n",
    "        # Turn on testing mode\n",
    "        with torch.no_grad():\n",
    "            rgbs, disps = render_path(render_poses, hwf, K, args.chunk, render_kwargs_test)\n",
    "        print('Done, saving', rgbs.shape, disps.shape)\n",
    "        moviebase = os.path.join(basedir, expname, '{}_spiral_{:06d}_'.format(expname, i))\n",
    "        imageio.mimwrite(moviebase + 'rgb.mp4', to8b(rgbs), fps=30, quality=8)\n",
    "        imageio.mimwrite(moviebase + 'disp.mp4', to8b(disps / np.max(disps)), fps=30, quality=8)\n",
    "\n",
    "            # if args.use_viewdirs:\n",
    "            #     render_kwargs_test['c2w_staticcam'] = render_poses[0][:3,:4]\n",
    "            #     with torch.no_grad():\n",
    "            #         rgbs_still, _ = render_path(render_poses, hwf, args.chunk, render_kwargs_test)\n",
    "            #     render_kwargs_test['c2w_staticcam'] = None\n",
    "            #     imageio.mimwrite(moviebase + 'rgb_still.mp4', to8b(rgbs_still), fps=30, quality=8)\n",
    "\n",
    "    if i%args.i_testset==0 and i > 0:\n",
    "        testsavedir = os.path.join(basedir, expname, 'testset_{:06d}'.format(i))\n",
    "        os.makedirs(testsavedir, exist_ok=True)\n",
    "        print('test poses shape', poses[i_test].shape)\n",
    "        with torch.no_grad():\n",
    "            render_path(torch.Tensor(poses[i_test]).to(device), hwf, K, args.chunk, render_kwargs_test, gt_imgs=images[i_test], savedir=testsavedir)\n",
    "        print('Saved test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddf9c43-2221-4525-aafe-e9b27f83c734",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    rgbs, disps = render_path(render_poses, hwf, K, args.chunk, render_kwargs_test)\n",
    "    print('Done, saving', rgbs.shape, disps.shape)\n",
    "    moviebase = os.path.join(basedir, expname, '{}_spiral_{:06d}_'.format(expname, i))\n",
    "    imageio.mimwrite(moviebase + 'rgb.mp4', to8b(rgbs), fps=30, quality=8)\n",
    "    imageio.mimwrite(moviebase + 'disp.mp4', to8b(disps / np.max(disps)), fps=30, quality=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96b7f537-6e13-453c-bead-69cb05f0af66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test poses shape torch.Size([3, 3, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                        | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.001312255859375\n",
      "torch.Size([378, 504, 3]) torch.Size([378, 504])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|██████████████████████████████████████████████████████████▋                                                                                                                     | 1/3 [00:11<00:23, 11.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 11.715906381607056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                          | 2/3 [00:24<00:12, 12.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 12.293184995651245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:36<00:00, 12.08s/it]\n"
     ]
    }
   ],
   "source": [
    "testsavedir = os.path.join(basedir, expname, 'testset_{:06d}'.format(i))\n",
    "os.makedirs(testsavedir, exist_ok=True)\n",
    "print('test poses shape', poses[i_test].shape)\n",
    "with torch.no_grad():\n",
    "    render_path(torch.Tensor(poses[i_test]).to(device), hwf, K, args.chunk, render_kwargs_test, gt_imgs=images[i_test], savedir=testsavedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb82670-90cb-4d8b-800d-65abde3eeb55",
   "metadata": {},
   "source": [
    "## Various Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "786b648a-a76b-468b-93d9-0d7063f7f213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import commentjson as json\n",
    "import tinycudann as tcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dbfc5c-27a3-4445-8896-cf74df52a371",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = rays_rgb[i_batch:i_batch+N_rand]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ec5bf9-bd88-41b2-aa7e-8bac6cd1c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.transpose(batch, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ca62fd-4195-4462-970f-ae9b409b8230",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aeb227-8787-4ba3-9fb1-f749fb1de68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_rays, target_s = batch[:2], batch[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e273ca96-dae6-41d8-8625-a84b5b6aaa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_batch += N_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fed3c16-ad84-4e08-af6a-fd8328bfd4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_rays.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5975b904-c342-4cc4-963f-0c0e4a3678b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk=args.chunk\n",
    "rays=batch_rays\n",
    "verbose=True\n",
    "retraw=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6eb6f1-ab29-46d1-a13a-d7b032631bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rays_o, rays_d = rays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dace8bd-c16e-4a2e-8bde-ad198b570ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh = rays_d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1c129d-e0c7-4208-9e00-c7198eca9169",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864ed0c3-ff8b-4938-a0de-35fb74513e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "rays_o, rays_d = ndc_rays(H, W, K[0][0], 1., rays_o, rays_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b65eed-991a-4b00-a386-7ad3685908fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rays_o = torch.reshape(rays_o, [-1,3]).float()\n",
    "rays_d = torch.reshape(rays_d, [-1,3]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be58e98f-fc35-44ee-af8b-48ba52c5547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rays_o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba26cf8-3d00-4665-af90-df8ecf851057",
   "metadata": {},
   "outputs": [],
   "source": [
    "near, far = near * torch.ones_like(rays_d[...,:1]), far * torch.ones_like(rays_d[...,:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111caea9-6ad7-4822-9eab-a9425e14eee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rays = torch.cat([rays_o, rays_d, near, far], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c236566b-291f-4646-83fd-b4146cb01d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_batch = rays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd6b9d7-05b1-495e-b8ed-46bb070b8397",
   "metadata": {},
   "outputs": [],
   "source": [
    "rays[:,-3:].shaperays_o, rays_d = ray_batch[:,0:3], ray_batch[:,3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3aaac8-b8d9-4ea8-9ae4-7318a5acd62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_batch[:,3:6]-rays_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44f67eb-b084-4471-b047-af6693c3671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = torch.reshape(ray_batch[...,6:8], [-1,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f01124-359a-4043-b00a-6d54e21750ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a631a05c-9084-4e9b-afe0-d91ce89a0b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.use_viewdirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec16fdf-1fb1-493c-a076-2146806c89bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewdirs = rays_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9244f80-9496-48f0-9592-b8d0dc14b503",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewdirs = viewdirs / torch.norm(viewdirs, dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f905e9-34e4-4d6f-9567-d3606d36268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewdirs = torch.reshape(viewdirs, [-1,3]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1409c7bf-1a37-4603-9e16-cc80bbd0aa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewdirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb78600-d91d-4567-81b9-ff1debfdb38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rays_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed248d8-b8ff-476c-b984-f4a23af89945",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeRF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f894195-7a6e-4cd3-a102-f9b5b39f4c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(rays_o,viewdirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a33e510-52a2-47e4-8b4d-279a1cf1fb85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
